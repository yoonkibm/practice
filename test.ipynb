{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello, how are you?', '안녕하세요, 어떻게 지내세요?'), ('What is your name?', '당신의 이름은 무엇인가요?'), ('I’m fine, thank you.', '잘 지내요, 감사합니다.'), ('Where are you from?', '어디에서 오셨나요?'), ('Please sit down.', '앉아주세요.'), ('Don’t touch that!', '그것을 만지지 마세요!'), ('Could you help me, please?', '도와주실 수 있나요?'), ('What time is it?', '지금 몇 시인가요?'), ('It’s 3 o’clock.', '3시입니다.'), ('How much does it cost?', '가격이 얼마인가요?'), ('It costs ten dollars.', '10달러입니다.'), ('Where is the nearest subway station?', '가장 가까운 지하철역은 어디인가요?'), ('I would like to go to the airport.', '공항으로 가고 싶어요.'), ('Do you have a map?', '지도 있나요?'), ('I’d like a coffee, please.', '커피 한 잔 주세요.'), ('Can I see the menu?', '메뉴를 볼 수 있을까요?'), ('Is this dish spicy?', '이 요리는 매운가요?'), ('I need to go to the supermarket.', '나는 슈퍼마켓에 가야 해요.'), ('The weather is nice today.', '오늘 날씨가 좋네요.'), ('I don’t feel well.', '몸이 좋지 않아요.'), ('I woke up at 7 AM.', '나는 오전 7시에 일어났어요.'), ('What are you doing this weekend?', '이번 주말에 뭐 하세요?'), ('See you tomorrow.', '내일 봐요.'), ('I am happy today.', '나는 오늘 행복해요.'), ('He is very angry.', '그는 매우 화가 났어요.'), ('I am a little tired.', '나는 조금 피곤해요.')]\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            src, tgt = line.strip().split(\"\\t\")\n",
    "            data.append((src, tgt))\n",
    "        return data\n",
    "    \n",
    "data =load_data(\"D:\\\\Users\\\\user\\\\git\\\\practice\\\\data\\\\en_kr.txt\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(data, tokenizer, max_length=128):\n",
    "    tokenized_data = []\n",
    "    for src,tgt in data:\n",
    "        src_tokens = tokenizer(src, max_length=max_length, padding = \"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        tgt_tokens = tokenizer(tgt, max_length=max_length, padding = \"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_data.append((src_tokens, tgt_tokens))\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_data = tokenize_data(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": src[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": src[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": tgt[\"input_ids\"].squeeze(0),\n",
    "        }\n",
    "    \n",
    "dataset = TextDataset(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader 생성\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# DataLoader 사용 예제\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)  # (batch_size, seq_len)\n",
    "    print(batch[\"labels\"].shape)    # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English unique words: 88\n",
      "Korean unique words: 69\n"
     ]
    }
   ],
   "source": [
    "# 고유 단어 수 계산\n",
    "english_words = set()\n",
    "korean_words = set()\n",
    "for src, tgt in data:\n",
    "    english_words.update(src.split())\n",
    "    korean_words.update(tgt.split())\n",
    "\n",
    "print(f\"English unique words: {len(english_words)}\")\n",
    "print(f\"Korean unique words: {len(korean_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 128, Position: 138\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋에서 가장 긴 문장의 길이 계산\n",
    "max_length = max(entry[\"input_ids\"].shape[0] for entry in dataset)\n",
    "\n",
    "# 여유 값을 더해 position 설정\n",
    "position = max_length + 10  # 여유로 10 추가\n",
    "print(f\"Max length: {max_length}, Position: {position}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=tokenizer.vocab_size\n",
    "d_model=256 \n",
    "position=138 \n",
    "num_heads=4\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer import Transformer\n",
    "transformer_model = Transformer(\n",
    "    vocab_size, \n",
    "    d_model, \n",
    "    position, \n",
    "    num_heads, \n",
    "    num_encoder_layers, \n",
    "    num_decoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 34925370\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1 completed. Average Loss: 2.7294\n",
      "Epoch 2/10\n",
      "Epoch 2 completed. Average Loss: 1.1366\n",
      "Epoch 3/10\n",
      "Epoch 3 completed. Average Loss: 0.9430\n",
      "Epoch 4/10\n",
      "Epoch 4 completed. Average Loss: 0.9115\n",
      "Epoch 5/10\n",
      "Epoch 5 completed. Average Loss: 0.9036\n",
      "Epoch 6/10\n",
      "Epoch 6 completed. Average Loss: 0.9046\n",
      "Epoch 7/10\n",
      "Epoch 7 completed. Average Loss: 0.9023\n",
      "Epoch 8/10\n",
      "Epoch 8 completed. Average Loss: 0.8989\n",
      "Epoch 9/10\n",
      "Epoch 9 completed. Average Loss: 0.9054\n",
      "Epoch 10/10\n",
      "Epoch 10 completed. Average Loss: 0.9018\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer_model = transformer_model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "transformer_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # 데이터를 GPU로 이동\n",
    "        inputs = data['input_ids'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "        # Optimizer gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 출력\n",
    "        outputs = transformer_model(inputs, labels, attention_mask)\n",
    "\n",
    "        # 차원 조정\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값 누적\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 1000번째 배치마다 손실 출력\n",
    "        if i % 1000 == 999:\n",
    "            batch_loss = running_loss / 1000\n",
    "            print(f\"  Batch {i+1}: Loss = {batch_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # 에포크 종료 시 평균 손실 출력\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} completed. Average Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = \"Hello, how are you?\"\n",
    "\n",
    "input = tokenizer(eng, max_length=max_length, padding = \"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "output_ids = [tokenizer.cls_token_id]\n",
    "transformer_model.eval()\n",
    "\n",
    "\n",
    "for i in range(max_length):\n",
    "    # output_ids를 입력 길이에 맞추어 패딩 추가\n",
    "    current_output_len = len(output_ids)\n",
    "    if current_output_len < input['input_ids'].size(1):  # 입력 길이보다 짧으면\n",
    "        padded_output_ids = output_ids + [tokenizer.pad_token_id] * (input['input_ids'].size(1) - current_output_len)\n",
    "    else:\n",
    "        padded_output_ids = output_ids\n",
    "\n",
    "    tgt_mask = torch.triu(torch.ones(max_length, max_length), diagonal=1).bool()\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    logit = transformer_model(input['input_ids'].to(device), torch.tensor(padded_output_ids, device=device).unsqueeze(0), tgt_mask)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logit[:,-1,:], dim=-1)\n",
    "\n",
    "    next_token = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    if next_token == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "    output_ids.append(next_token)\n",
    "\n",
    "krn = tokenizer.decode(output_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
